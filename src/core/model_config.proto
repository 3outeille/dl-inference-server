// Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
//
// Redistribution and use in source and binary forms, with or without
// modification, are permitted provided that the following conditions
// are met:
//  * Redistributions of source code must retain the above copyright
//    notice, this list of conditions and the following disclaimer.
//  * Redistributions in binary form must reproduce the above copyright
//    notice, this list of conditions and the following disclaimer in the
//    documentation and/or other materials provided with the distribution.
//  * Neither the name of NVIDIA CORPORATION nor the names of its
//    contributors may be used to endorse or promote products derived
//    from this software without specific prior written permission.
//
// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
// EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
// IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
// PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
// CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
// EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
// PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
// OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

syntax = "proto3";

package nvidia.inferenceserver;

// Data types supported for input and output tensors.
enum DataType {
  TYPE_INVALID = 0;

  TYPE_BOOL = 1;

  TYPE_UINT8 = 2;
  TYPE_UINT16 = 3;
  TYPE_UINT32 = 4;
  TYPE_UINT64 = 5;

  TYPE_INT8 = 6;
  TYPE_INT16 = 7;
  TYPE_INT32 = 8;
  TYPE_INT64 = 9;

  TYPE_FP16 = 10;
  TYPE_FP32 = 11;
  TYPE_FP64 = 12;
}

// An instance of a model and resources made available for that
// instance
message ModelInstance {
  // Optional name of this instance. If not specified the instance
  // name will be formed as <model name>_<instance number>.
  string name = 1;

  // GPU(s) made available for this instance given as indices in the
  // manner of CUDA_VISIBLE_DEVICES. If not specified, no GPUs will be
  // made available to the instance.
  repeated int32 gpus = 2;
}

// Input tensor for the model
message ModelInput {
  // Format for the input.
  enum Format {
    // The input has no specific format.
    FORMAT_NONE = 0;

    // Image formats. Tensors with this format require 3 dimensions if
    // the model does not support batching (max_batch_size = 0) or 4
    // dimensions if the model does support batching (max_batch_size
    // >= 1). In either case the 'dims' below should only specify the
    // 3 non-batch dimensions (i.e. HWC or CHW).
    FORMAT_NHWC = 1;
    FORMAT_NCHW = 2;
  }

  string name = 1;
  DataType data_type = 2;
  Format format = 3;
  repeated int64 dims = 4;
}

// Output tensor for the model
message ModelOutput {
  string name = 1;
  DataType data_type = 2;
  repeated int64 dims = 3;

  // Label file for this output (optional).
  string label_filename = 4;
}

// Model configuration.
message ModelConfig {
  // Name of the model.
  string name = 1;

  // Base path to the model, excluding the version directory. For
  // example, for a model at /foo/bar/my_model/123, where 123 is the
  // version, the base path is /foo/bar/my_model.
  string base_path = 2;

  // Type of model (e.g. "tensorflow").
  string model_platform = 3;

  // Maximum batch size allowed for inference. This can only decrease
  // what is allowed by the model itself. A value of 0 indicates that
  // batching is not-allowed/is-disabled (for some input formats this
  // has implications on the expected dimension of the inputs, see
  // Format above).
  int32 max_batch_size = 4;

  // Inputs and outputs to the model.
  repeated ModelInput input = 5;
  repeated ModelOutput output = 6;

  // Instances of this model. For each instance the GPU(s) that should
  // be made available for that instance given as indices in the
  // manner of CUDA_VISIBLE_DEVICES. If not specified, one instance of
  // the model will be instantiated and GPU index 0 will be made
  // available to the instance.
  repeated ModelInstance instance = 7;
}

// List of model configurations.
message ModelConfigList {
  repeated ModelConfig config = 1;
}
